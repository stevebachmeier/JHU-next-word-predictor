--------------------------------------------------------------------------------------------------------------------------------------------
QUESTIONS:
--------------------------------------------------------------------------------------------------------------------------------------------
[] Should I remove n-grams already found in higher order n-grams (eg remove "do I do" if "what do I do" existed)?
[] How can I speed up building the lists of ngrams?
[] 

--------------------------------------------------------------------------------------------------------------------------------------------
SUGGESTIONS:
--------------------------------------------------------------------------------------------------------------------------------------------
[x] Find an R profiler to hunt for bottlenecks.
[] Remove words found in 2+ grams from the unigrams (to prevent repeated words, eg I have a a)
[x] try perl=T in grep()
[x] try grep(fixed=T) to search for fixed strings instead of regular expressions
	Figure out warnings. Why do ngrams_matched and preds have different lengths sometimes (ie backoff 2 and 3)
	- The reason: fixed=T finds extra things, eg when searching for "is_" it also includes "this_"
[x] Remove low-enough matches (maybe with freq <= 3?)
[x] Try subsetting data tables instead of grepping?
[] Filter out swear words
[x] BUG 01: applying S-values to non-zero predictions (preds_bo2[preds_bo2$pred %in% ngrams_matched_bo2$pred,]$S <- ngrams_matched_bo2$S) 
	is not working properly
[x] BUG 02: Cannot merge preds and ngrams_matched if ngrams_matched remains empty (ie if user words <3)
[] Can indexing my all_ngrams files speed things up? ie extract last word of ngram and make a new column of that. Then index on that?
[x] (optional) Consider expanding to 4- or 5-grams to increase accuracy.
[] Create one preds table and append appropriate columns rather than different tables each search
[x] Test different ngrams freq filtering (default has been 3)
[x] Test accuracies of different sampling sizes

--------------------------------------------------------------------------------------------------------------------------------------------
PROFILING:
--------------------------------------------------------------------------------------------------------------------------------------------
Tried (nlpPredictor("Hey! How long will this"))
1. Original/BASELINE: 3.14s, bottleneck: grep (89.2%)
2. grep(..., perl=T): 2.18s (-31%), bottlenecks: grep (82.6%)
3. grep(..., perl=F, fixed=T): 
	* 1.68s (-46.5%)
	* bottlenecks: grep (72.6%)
		- There are warnings, though!
4. #3 with n-grams >= 3
	* 0.34s (-89.2%)
	* bottlenecks: data.table (41.2%, sys.call (23.5%)
		- This gave different results - need to start considering accuracy!
		- The bottlenecks appear to be changing every time I re-run the same test.
			- This may mean that there's no real single bottleneck?
5. ngrams_matched <- all_4grams[grep(search_string, all_4grams$ngram, perl=T),] --> ngrams_matched <- all_4grams[ngram %like% search_string]:
	0.4s - slightly slower, about the same time on repeated runs
	bottlenecks: changes depending on run: grepl/alloc.col, grepl/unique
	* go back to grep approach
6. Append '*' to beginning of n-grams to signify start of ngram (to fix problem with, eg, searching for 'is_time_' matching 'this_time_' with	
	grep fixed=T
	* 0.22s (-93%)
	* bottlenecks: grep, str_replace_first_regex
		- NOTE: There is a bug (bug 01) here with creating preds data tables 
		(eg preds_bo2[preds_bo2$pred %in% ngrams_matched_bo2$pred,]$S <- ngrams_matched_bo2$S): doesn't properly order S values
7. Fixed Bug01: Merged preds with ngrams_matched instead of just using %in%.
	* 0.34s (-89.2%) - slower than %in% but still pretty fast.
	* bottlenecks: unique.default, bmerge
8. Fixed Bug02: Added if statements when building pred to check if nrow(ngrams_matched)=0
	* 0.42s - a bit slower.
	* bottlenecks: unique.default, bmerge
9. 4gram model
	* 0.28s - faster for some reason
	* bottlenecks: bmerge, grep, unique, [.data.table]
10. 5gram model
	* 0.3-0.44s - slower. Accuracy?
	* bottlenecks: grep, bmerge

--------------------------------------------------------------------------------------------------------------------------------------------
BENCHMARKING:
Using benchmark.R found on github and the forum (password=capstone4)
--------------------------------------------------------------------------------------------------------------------------------------------
RESULTS from below.
1. 3grams, 4grams, and 5grams models all have same accuracy. Just stick with 3grams.
2. Having a single preds table instead of separate ones was slower (0.237s vs 0.218s).
3. Tweaked for slightly faster speed (210ms vs 218ms)

1. Compare 3grams, 4grams, and 5grams models; top 30 entries each for blogs and tweets
	* f_nlpPredictor_3grams.R
		Overall top-3 score:     16.77 %
		Overall top-1 precision: 13.33 %
		Overall top-3 precision: 19.78 %
		Average runtime:         218.31 msec
		Number of predictions:   1226
		Total memory used:       74.56 MB
	
	* f_nlpPredictor_4grams.R
		Overall top-3 score:     16.77 %
		Overall top-1 precision: 13.33 %
		Overall top-3 precision: 19.78 %
		Average runtime:         239.67 msec
		Number of predictions:   1226
		Total memory used:       78.31 MB			

	* f_nlpPredictor_5grams.R
		Overall top-3 score:     16.77 %
		Overall top-1 precision: 13.33 %
		Overall top-3 precision: 19.78 %
		Average runtime:         285.14 msec
		Number of predictions:   1226
		Total memory used:       79.63 MB
		  
2. Compare one preds table vs separate; top 30 each tweets and blogs
	* f_nlpPredictor_3grams_WORKING.R 
		Overall top-3 score:     16.77 %
		Overall top-1 precision: 13.33 %
		Overall top-3 precision: 19.78 %
		Average runtime:         237.31 msec
		Number of predictions:   1226
		Total memory used:       74.50 MB
		  
3. Tweaked preds buildup to remove a couple of column renaming operations; top 30 lines
		Overall top-3 score:     16.77 %
		Overall top-1 precision: 13.33 %
		Overall top-3 precision: 19.78 %
		Average runtime:         210.92 msec
		Number of predictions:   1226
		Total memory used:       74.54 MB
		  
4. Tried filtering to freq=10 instead of 3; top 20 lines
Result: 51% faster; 15.6% less accurate
	* freq=10
		Overall top-3 score:     14.38 %
		Overall top-1 precision: 11.61 %
		Overall top-3 precision: 16.76 %
		Average runtime:         237.82 msec
		Number of predictions:   758
		Total memory used:       17.72 MB
	* freq=3, first 20 each of tweets and blogs
		Overall top-3 score:     17.03 %
		Overall top-1 precision: 14.00 %
		Overall top-3 precision: 19.53 %
		Average runtime:         490.17 msec
		Number of predictions:   758
		Total memory used:       74.79 MB
		
5. Compared 10% corpus sampling to 20% corpus sampling; top 20 lines
Results: 0.3% less accurate; 34% faster
	* 10%
		Overall top-3 score:     14.38 %
		Overall top-1 precision: 11.61 %
		Overall top-3 precision: 16.76 %
		Average runtime:         77.35 msec
		Number of predictions:   758
		Total memory used:       17.73 MB
	* 20%
		Overall top-3 score:     14.43 %
		Overall top-1 precision: 11.48 %
		Overall top-3 precision: 17.02 %
		Average runtime:         117.61 msec
		Number of predictions:   758
		Total memory used:       33.82 MB
6. Compare a 5% corpus sampling to a 10%; top 20 lines
Results: 7% less accurate, 31% faster
	* 5%
		Overall top-3 score:     13.37 %
		Overall top-1 precision: 10.82 %
		Overall top-3 precision: 15.70 %
		Average runtime:         51.53 msec
		Number of predictions:   758
		Total memory used:       9.78 MB
	* 10%
		Overall top-3 score:     14.38 %
		Overall top-1 precision: 11.61 %
		Overall top-3 precision: 16.76 %
		Average runtime:         74.45 msec
		Number of predictions:   758
		Total memory used:       17.73 MB

7. Compare freq=6 to freq=10; 5% sampling, top 20 lines
Results: 4% more accurate; 26% slower
	* freq=6:
		Overall top-3 score:     13.90 %
		Overall top-1 precision: 11.22 %
		Overall top-3 precision: 16.36 %
		Average runtime:         73.11 msec
		Number of predictions:   758
		Total memory used:       16.22 MB
	* freq=10:
		Overall top-3 score:     13.37 %
		Overall top-1 precision: 10.82 %
		Overall top-3 precision: 15.70 %
		Average runtime:         58.22 msec
		Number of predictions:   758
		Total memory used:       9.78 MB
8. Compare freq=3 to freq=4; 5% sampling; top 20 lines
Results: 1.2% more accurate; 30% slowerr
	* freq=3:
		Overall top-3 score:     14.69 %
		Overall top-1 precision: 11.88 %
		Overall top-3 precision: 17.29 %
		Average runtime:         135.45 msec
		Number of predictions:   758
		Total memory used:       38.28 MB
	* freq=4:
		Overall top-3 score:     14.52 %
		Overall top-1 precision: 11.75 %
		Overall top-3 precision: 17.16 %
		Average runtime:         104.38 msec
		Number of predictions:   758
		Total memory used:       25.98 MB
		
9. Compare freq=1 to freq=10; 5% sampling; top 20 lines
Results: 14% more accurate; 1,567% slower
	* freq=1
		Overall top-3 score:     15.23 %
		Overall top-1 precision: 12.54 %
		Overall top-3 precision: 17.69 %
		Average runtime:         1026.45 msec
		Number of predictions:   758
		Total memory used:       765.65 MB
	* freq=10
		Overall top-3 score:     13.37 %
		Overall top-1 precision: 10.82 %
		Overall top-3 precision: 15.70 %
		Average runtime:         61.56 msec
		Number of predictions:   758
		Total memory used:       9.78 MB
		
10. Compare freq=2 vs freq=3; 10% sampling; top 20 lines
Results: 1% more accurate; 67% slower
	* freq=2
		Overall top-3 score:     17.20 %
		Overall top-1 precision: 14.13 %
		Overall top-3 precision: 19.53 %
		Average runtime:         374.23 msec
		Number of predictions:   758
		Total memory used:       149.60 MB
	* freq=3
		Overall top-3 score:     17.03 %
		Overall top-1 precision: 14.00 %
		Overall top-3 precision: 19.53 %
		Average runtime:         224.02 msec
		Number of predictions:   758
		Total memory used:       74.79 MB
		
11. Compare freq=3 vs freq=4; 10% sampling; top 20 lines
Results: 12% increase in accuracy, 25% reduction in speed.
	This is the balance point! Accuracy? Go with freq=3. Speed? go with freq=4.
	Freq=3 already works well enough online, so stick with that.
	* freq=3
		Overall top-3 score:     17.03 %
		Overall top-1 precision: 14.00 %
		Overall top-3 precision: 19.53 %
		Average runtime:         247.37 msec
		Number of predictions:   758
		Total memory used:       74.79 MB
	* freq=4
		Overall top-3 score:     15.09 %
		Overall top-1 precision: 12.27 %
		Overall top-3 precision: 17.68 %
		Average runtime:         197.57 msec
		Number of predictions:   758
		Total memory used:       50.22 MB

11. Compare 25% sample to 10% sample; freq=3; top 20 lines
Results: .82 LESS accurate; 84% slower -> obvviously go with 10% sample.
	* 25% sample
		Overall top-3 score:     16.89 %
		Overall top-1 precision: 13.85 %
		Overall top-3 precision: 19.39 %
		Average runtime:         451.57 msec
		Number of predictions:   758
		Total memory used:       185.64 MB
	* 10% sample
		Overall top-3 score:     17.03 %
		Overall top-1 precision: 14.00 %
		Overall top-3 precision: 19.53 %
		Average runtime:         244.97 msec
		Number of predictions:   758
		Total memory used:       74.79 MB

